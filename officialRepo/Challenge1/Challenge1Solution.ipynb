{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv('data/Challenge1_Training_Scenarios.csv')\n",
    "df_train.set_index('scenario_id', inplace=True)\n",
    "\n",
    "# development\n",
    "X = df_train['scenario'].to_numpy()\n",
    "y = df_train['danger_level'].to_numpy()\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# submission\n",
    "# train_X = list(df_train['scenario'])\n",
    "# train_y = list(df_train['danger_level'])\n",
    "\n",
    "# df_test = pd.read_csv('data/Challenge1_Test_Scenarios.csv')\n",
    "# test_X = list(df_test['scenario'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_ratio(y, predictions):\n",
    "    zipped = zip(y, predictions)\n",
    "    total_points = 0\n",
    "    for curr in zipped:\n",
    "        if curr[0] == curr[1]:\n",
    "            total_points += 2\n",
    "        elif curr[0] == curr[1]+1 or curr[0] == curr[1]-1:\n",
    "            total_points += 1\n",
    "        else:\n",
    "            total_points += 0\n",
    "\n",
    "    return (total_points/(len(y)*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_predictions(X, y, pred):\n",
    "    visualize = pd.DataFrame(list(zip(list(X), y, list(pred))))\n",
    "    visualize['diff'] = list(np.subtract(y, pred))\n",
    "    visualize = visualize.sort_values(by = 'diff')\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "    print(visualize.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "def remove_punctuation_and_lower(text):\n",
    "    text = re.sub(r\"[{}]\".format(string.punctuation), \" \", text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bows\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "vect = TfidfVectorizer(preprocessor=remove_punctuation_and_lower, tokenizer=LemmaTokenizer(), ngram_range=(1,3))\n",
    "train_counts = vect.fit_transform(train_X)\n",
    "test_counts = vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning for models being tested below\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bow_mnb = MultinomialNB(alpha=0.1)\n",
    "#parameters = {'alpha': (1,0.1,0.01,0.001,0.0001,0.00001)}\n",
    "#bow_mnb = GridSearchCV(MultinomialNB(), parameters)\n",
    "\n",
    "bow_mnb.fit(train_counts, train_y)\n",
    "pred_bow_mnb = bow_mnb.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_mnb))\n",
    "#bow_mnb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "bow_lr = LinearRegression()\n",
    "#parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True,False]}\n",
    "#bow_lr = GridSearchCV(LinearRegression(), parameters)\n",
    "\n",
    "bow_lr.fit(train_counts, train_y)\n",
    "pred_bow_lr = np.rint(bow_lr.predict(test_counts))\n",
    "print(accuracy_ratio(test_y, pred_bow_lr))\n",
    "#bow_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bow_kn = KNeighborsClassifier(n_neighbors=30, weights='distance', metric='euclidean')\n",
    "#parameters = {'n_neighbors':[3,5,11,15,30,50], 'weights':['uniform','distance'], 'metric':['euclidean','manhattan','minkowski']}\n",
    "#bow_kn = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "\n",
    "bow_kn.fit(train_counts, train_y)\n",
    "pred_bow_kn = bow_kn.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_kn))\n",
    "#bow_kn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "bow_svc = SVC(C=10, kernel='linear')\n",
    "#parameters = {'C':[0.1,1,10,50] , 'gamma':['scale','auto',0.1,1], 'kernel':['linear','rbf','sigmoid']}\n",
    "#bow_svc = GridSearchCV(SVC(), parameters)\n",
    "\n",
    "bow_svc.fit(train_counts, train_y)\n",
    "pred_bow_svc = bow_svc.predict(test_counts)\n",
    "print(accuracy_ratio(test_y, pred_bow_svc))\n",
    "#bow_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view_predictions(test_X, test_y, pred_bow_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "def tokenize_remove_punctuation(input):\n",
    "    input = input.lower()\n",
    "    input = word_tokenize(input)\n",
    "    input = list(filter(lambda token: token not in string.punctuation, input))\n",
    "    return input\n",
    "\n",
    "def create_embedded(input):\n",
    "    embedded = input.copy()\n",
    "    for i, entry in enumerate(embedded):\n",
    "        embedded[i] = tokenize_remove_punctuation(embedded[i])\n",
    "\n",
    "        if 'covid' in embedded[i]:\n",
    "            embedded[i][embedded[i].index('covid')] = 'coronavirus'\n",
    "\n",
    "        for j, word in enumerate(embedded[i]):\n",
    "            if embedded[i][j] in glove_vectors:\n",
    "                embedded[i][j] = glove_vectors[embedded[i][j]]\n",
    "            else:\n",
    "                embedded[i][j] = None\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab embeddings for valued words and format\n",
    "valued_words = ['mask', 'coronavirus', 'travel', 'home', 'outside', 'asthma']\n",
    "\n",
    "for i, word in enumerate(valued_words):\n",
    "    valued_words[i] = glove_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to embeddings for each entry\n",
    "train_embedded_X = create_embedded(train_X) \n",
    "test_embedded_X = create_embedded(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_min_distances(embedding, valued_words):\n",
    "    output = []\n",
    "    for i, entry in enumerate(embedding):\n",
    "        curr_min_distance_vec = np.full(len(valued_words), float('inf'))\n",
    "\n",
    "        # for each word\n",
    "        for j, word in enumerate(embedding[i]):\n",
    "            if word is not None:\n",
    "                # loop through valued words\n",
    "                for k, valued_word in enumerate(valued_words):\n",
    "                    curr_distance = np.sum(np.square(valued_word - word))\n",
    "                    if curr_distance < curr_min_distance_vec[k]:\n",
    "                        curr_min_distance_vec[k] = curr_distance\n",
    "        output.append(curr_min_distance_vec.copy())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_min_distances = calculate_min_distances(train_embedded_X, valued_words)\n",
    "test_min_distances = calculate_min_distances(test_embedded_X, valued_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_mnb = MultinomialNB()\n",
    "distances_mnb.fit(train_min_distances, train_y)\n",
    "pred_distances_mnb = distances_mnb.predict(test_min_distances)\n",
    "print(accuracy_ratio(test_y, pred_distances_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_lr = LinearRegression()\n",
    "distances_lr.fit(train_min_distances, train_y)\n",
    "pred_distances_lr = np.rint(distances_lr.predict(test_min_distances))\n",
    "print(accuracy_ratio(test_y, pred_distances_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_svc = SVC()\n",
    "distances_svc.fit(train_min_distances, train_y)\n",
    "pred_distances_svc = distances_svc.predict(test_min_distances)\n",
    "print(accuracy_ratio(test_y, pred_distances_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row\n",
    "def calculate_avg_embedding(embedding):\n",
    "    output = []\n",
    "    for i, entry in enumerate(embedding):\n",
    "        row_total = np.zeros(embedding[i][0].shape)\n",
    "        for j, word in enumerate(embedding[i]):\n",
    "            if word is not None:\n",
    "                row_total = np.sum([embedding[i][j], row_total], axis=0)\n",
    "        output.append(row_total)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_embed_X = calculate_avg_embedding(train_embedded_X)\n",
    "test_avg_embed_X = calculate_avg_embedding(test_embedded_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lr = LinearRegression()\n",
    "avg_lr.fit(train_avg_embed_X, train_y)\n",
    "pred_avg_lr = np.rint(avg_lr.predict(test_avg_embed_X))\n",
    "print(accuracy_ratio(test_y, pred_avg_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_svc = SVC()\n",
    "avg_svc.fit(train_avg_embed_X, train_y)\n",
    "pred_avg_svc = avg_svc.predict(test_avg_embed_X)\n",
    "print(accuracy_ratio(test_y, pred_avg_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembling predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_majority(prediction_list, index):\n",
    "    prediction = [pred[index] for pred in prediction_list]\n",
    "    majority = Counter(prediction).most_common()\n",
    "    return majority[0][0]\n",
    "\n",
    "def predict_ensemble(prediction_list):\n",
    "    ensemble_predictions = []\n",
    "    for i, curr_pred in enumerate(prediction_list[0]):\n",
    "        ensemble_predictions.append(get_majority(prediction_list, i))\n",
    "    return ensemble_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [pred_bow_mnb, pred_bow_lr, pred_bow_kn, pred_bow_svc, pred_distances_mnb, pred_distances_lr, pred_distances_svc, pred_avg_lr, pred_avg_svc]\n",
    "ensemble_predictions = predict_ensemble(test_predictions)\n",
    "ensemble_predictions = [round(x) for x in ensemble_predictions]\n",
    "print(accuracy_ratio(test_y, ensemble_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output code if creating submission\n",
    "# output = pd.DataFrame(zip(list(df_test['scenario_id']), ensemble_predictions), columns=['scenario_id','danger_level'])\n",
    "# output.to_csv('submission/Challenge1_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
